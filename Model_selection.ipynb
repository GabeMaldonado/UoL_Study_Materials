{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Model selection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GabeMaldonado/UoL_Study_Materials/blob/main/Model_selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtRSNNuAhgZ5"
      },
      "source": [
        "# Model Selection\n",
        "\n",
        "## Instructions:\n",
        "* Go through the notebook and complete the tasks.\n",
        "* Make sure you understand the examples given. If you need help, refer to the documentation links provided or go to the Topic 4 discussion forum..\n",
        "* When a question allows a free-form answer (e.g. what do you observe?), create a new markdown cell below and answer the question in the notebook. \n",
        "* Save your notebooks when you are done. \n",
        "\n",
        "In this lab, we will work through using cross-validation.\n",
        "\n",
        "**Task 1:**\n",
        "In the cell below, we load the iris dataset and show the indices recovered by using a built-in cross-validation function from scikit-learn. These indices can be then used for splitting our data into a training set and testing set. Note that while in this example we use StratifiedShuffleSplit, there are many other cross-validation generators in scikit-learn. You can read more about this <a href=\"https://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html\">here</a>.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTIiUWK5hgZ-",
        "outputId": "df5dd2c3-18ed-4d85-f98a-7c5bb96bbef5"
      },
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "#Set X a samples times features matrix, Y equal to the targets\n",
        "#use only the first 10 datapoints for this example\n",
        "X=iris.data[0:10] \n",
        "y=iris.target[0:10] \n",
        "\n",
        "#we use 10 splits, with the test size being 0.2\n",
        "cvsplt = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
        "\n",
        "for train_index, test_index in cvsplt.split(X, y):\n",
        "    print(\"train indices:\", train_index, \"test indices:\", test_index)\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train indices: [9 8 4 3 6 2 7 1] test indices: [5 0]\n",
            "train indices: [1 6 8 3 0 2 4 5] test indices: [9 7]\n",
            "train indices: [8 7 3 5 1 9 4 6] test indices: [0 2]\n",
            "train indices: [5 0 9 8 6 2 3 1] test indices: [4 7]\n",
            "train indices: [5 4 6 0 1 2 7 3] test indices: [8 9]\n",
            "train indices: [8 6 2 0 7 3 1 5] test indices: [4 9]\n",
            "train indices: [7 8 5 3 9 6 4 2] test indices: [1 0]\n",
            "train indices: [4 8 5 1 6 7 9 3] test indices: [2 0]\n",
            "train indices: [8 9 0 4 6 1 5 7] test indices: [3 2]\n",
            "train indices: [6 2 4 1 7 8 9 5] test indices: [3 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjPtgTmHhgaC"
      },
      "source": [
        "**Task 2:**\n",
        "In the cell below, we have the sample code that was used to train a k-NN classifier on the iris dataset. Your task is to use the code above to change the classification process we show below, in order to perform cross-validation on the data and obtain an estimate of the performance of the classifier with 10 neighbours on the unseen data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb8gMK4ghgaC",
        "outputId": "14c7d4c7-136d-4839-f455-1f1020da7c45"
      },
      "source": [
        "from sklearn import datasets\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "iris = datasets.load_iris()\n",
        "#Set X a samples times features matrix, Y equal to the targets\n",
        "X=iris.data \n",
        "y=iris.target \n",
        "\n",
        "\n",
        "#split to train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "#define knn classifier, with 5 neighbors and use the euclidian distance\n",
        "knn=KNeighborsClassifier(n_neighbors=10, metric='euclidean')\n",
        "#define training and testing data, fit the classifier\n",
        "knn.fit(X_train,y_train)\n",
        "#predict values for test data based on training data\n",
        "y_pred=knn.predict(X_test)\n",
        "#print values\n",
        "print(y_test) # true values\n",
        "print(y_pred) # predicted values\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 1 2 2 2 0 1 2 1 0 0 0 0 1 2 0 2 2 0 2 2 0 1 1 2 0 2 1 2]\n",
            "[1 1 1 1 2 2 0 1 2 1 0 0 0 0 1 2 0 2 2 0 2 2 0 1 1 2 0 2 1 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XA0LtNt8lCYk",
        "outputId": "e7456ef5-61f1-4710-845e-f9cbccc92170"
      },
      "source": [
        "from sklearn import datasets\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "iris = datasets.load_iris()\n",
        "#Set X a samples times features matrix, Y equal to the targets\n",
        "X=iris.data \n",
        "y=iris.target \n",
        "\n",
        "for train_index, test_index in cvsplt.split(X, y):\n",
        "    #print(\"train indices:\", train_index, \"test indices:\", test_index)\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # instantiate the KNN classifier\n",
        "    knn = KNeighborsClassifier(n_neighbors=10, metric='euclidean')\n",
        "    knn.fit(X_train, y_train)\n",
        "    y_pred = knn.predict(X_test)\n",
        "    print(metrics.accuracy_score(y_test, y_pred))\n",
        "\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "1.0\n",
            "0.9\n",
            "0.9666666666666667\n",
            "0.9666666666666667\n",
            "0.9\n",
            "1.0\n",
            "0.9333333333333333\n",
            "0.9666666666666667\n",
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmHwMKU8hgaD"
      },
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8njdZuqUhgaD"
      },
      "source": [
        "\n",
        "**Task 3:**  Now you are going to attempt to write your own cross-validation function to evaluate the k-NN classifier above (i.e. not using StratifiedShuffleSplit). \n",
        "\n",
        "Again, we are using a fixed distance (euclidean) and a fixed number of neighbours (10) so we do **not** need to create a validation set.\n",
        "\n",
        "Your function (see cell below) firstly splits the indices of each of our data into bins according to the number of folds (here: 5-fold).\n",
        "\n",
        "Then, you should loop through all folds, split the data into training and testing by selecting the appropriate bins (see slides on cross-validation), train on training data and save the test result as the accuracy for each fold (see list accuracy_fold). This is the list that your function should return in the end. Remember that the extend function extends a list with more values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-UAxQ0ghgaD",
        "outputId": "32753a78-d3a5-46b5-f316-aa64ba845aba"
      },
      "source": [
        "def myCrossVal(X,y,foldK):\n",
        "  from sklearn.neighbors import KNeighborsClassifier\n",
        "  \n",
        "  accuracy_fold=[] #list to store accuracies folds\n",
        "  \n",
        "  \n",
        "  #TASK: use the function np.random.permutation to generate a list of shuffled indices from in the range (0,number of data)\n",
        "  np.random.seed(0)\n",
        "\n",
        "  # create an array of random permutation of indices between 0 and the len(X)\n",
        "  indices = np.random.permutation(np.arange(0, len(X), 1))\n",
        "  print(indices)\n",
        "  \n",
        "  #TASK: use the function array_split to split the indices to k different bins:\n",
        "  bins = np.array_split(indices, foldK)\n",
        "  print(bins)\n",
        "  \n",
        "  \n",
        "  #loop through folds\n",
        "  for i in range(0,foldK):\n",
        "      foldTrain=[] # list to save current indices for training\n",
        "      foldTest=[]  # list to save current indices for testing\n",
        "      \n",
        "      #TASK: take bin i for testing, rest for training.  Can use the function extend to add indices to foldTrain and foldTest\n",
        "      foldTest = bins[i]\n",
        "\n",
        "      for j in range(0, foldK):\n",
        "        if j!= i:\n",
        "          foldTrain.extend(bins[j])\n",
        "\n",
        "      #train kNN classifier\n",
        "      knn = KNeighborsClassifier(n_neighbors=10, metric='euclidean')\n",
        "      knn.fit(X[foldTrain,:], y[foldTrain])\n",
        "\n",
        "      #test on test data\n",
        "      y_pred = knn.predict(X[foldTest,:])\n",
        "\n",
        "      #append the new accuracy to your accuracy_fold list.  You can use accuracy_score or your myAccuracy function.\n",
        "      accuracy_fold.append(metrics.accuracy_score(y[foldTest], y_pred))\n",
        "\n",
        "  return accuracy_fold;\n",
        "  \n",
        "accuracy_fold=myCrossVal(X,y,5)\n",
        "print(accuracy_fold)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[114  62  33 107   7 100  40  86  76  71 134  51  73  54  63  37  78  90\n",
            "  45  16 121  66  24   8 126  22  44  97  93  26 137  84  27 127 132  59\n",
            "  18  83  61  92 112   2 141  43  10  60 116 144 119 108  69 135  56  80\n",
            " 123 133 106 146  50 147  85  30 101  94  64  89  91 125  48  13 111  95\n",
            "  20  15  52   3 149  98   6  68 109  96  12 102 120 104 128  46  11 110\n",
            " 124  41 148   1 113 139  42   4 129  17  38   5  53 143 105   0  34  28\n",
            "  55  75  35  23  74  31 118  57 131  65  32 138  14 122  19  29 130  49\n",
            " 136  99  82  79 115 145  72  77  25  81 140 142  39  58  88  70  87  36\n",
            "  21   9 103  67 117  47]\n",
            "[array([114,  62,  33, 107,   7, 100,  40,  86,  76,  71, 134,  51,  73,\n",
            "        54,  63,  37,  78,  90,  45,  16, 121,  66,  24,   8, 126,  22,\n",
            "        44,  97,  93,  26]), array([137,  84,  27, 127, 132,  59,  18,  83,  61,  92, 112,   2, 141,\n",
            "        43,  10,  60, 116, 144, 119, 108,  69, 135,  56,  80, 123, 133,\n",
            "       106, 146,  50, 147]), array([ 85,  30, 101,  94,  64,  89,  91, 125,  48,  13, 111,  95,  20,\n",
            "        15,  52,   3, 149,  98,   6,  68, 109,  96,  12, 102, 120, 104,\n",
            "       128,  46,  11, 110]), array([124,  41, 148,   1, 113, 139,  42,   4, 129,  17,  38,   5,  53,\n",
            "       143, 105,   0,  34,  28,  55,  75,  35,  23,  74,  31, 118,  57,\n",
            "       131,  65,  32, 138]), array([ 14, 122,  19,  29, 130,  49, 136,  99,  82,  79, 115, 145,  72,\n",
            "        77,  25,  81, 140, 142,  39,  58,  88,  70,  87,  36,  21,   9,\n",
            "       103,  67, 117,  47])]\n",
            "[1.0, 0.8666666666666667, 1.0, 0.9666666666666667, 0.9666666666666667]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsL1BIMkhgaE"
      },
      "source": [
        "**Task 4:** Print the average accuracy and standard deviation of your results over the 5 folds. (functions ``mean`` and ``std``)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIDuTOLkhgaE",
        "outputId": "f517be0c-47d4-4428-bc2f-c56bcd73bbcf"
      },
      "source": [
        "print(\"Average accuracy: %.3f (std. %.3f))\" % (np.mean(accuracy_fold), np.std(accuracy_fold)))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average accuracy: 0.960 (std. 0.049))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1tMNl8RhgaF"
      },
      "source": [
        "### Nested cross validation\n",
        "Performing **nested** cross validation is slightly more elaborate, but necessary when we want to learn the best values for our parameters (e.g. here, finding the best values for the number of neighbours that k-NN should have). You can follow the examples on scikit-learn <a href=\"https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html\">here</a> and <a href=\"https://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html\">here</a> to learn how to do this.\n"
      ]
    }
  ]
}